"""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π ETL –ø—Ä–æ—Ü–µ—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –Ø–Ω–¥–µ–∫—Å.–ú–µ—Ç—Ä–∏–∫–∏ –≤ ClickHouse
–í–µ—Ä—Å–∏—è: 2.0 (—Ä–µ—Ñ–∞–∫—Ç–æ—Ä–µ–Ω–Ω–∞—è)

–û—Å–Ω–æ–≤–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
- –ò—Å–ø—Ä–∞–≤–ª–µ–Ω—ã SQL-–∏–Ω—ä–µ–∫—Ü–∏–∏ —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
- –î–æ–±–∞–≤–ª–µ–Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- –£–ª—É—á—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
- –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ —Ä–∞–±–æ—Ç–∞ —Å –ø–∞–º—è—Ç—å—é
- –î–æ–±–∞–≤–ª–µ–Ω—ã —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏
- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤—ã–Ω–µ—Å–µ–Ω–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–æ–¥—É–ª–∏
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from airflow.utils.trigger_rule import TriggerRule
from datetime import datetime, timedelta
import logging
import os
import time
import gc
from typing import List, Dict, Any, Optional, Tuple
from contextlib import contextmanager
from dataclasses import dataclass, field
from enum import Enum
import clickhouse_connect
from tapi_yandex_metrika import YandexMetrikaLogsapi
from dateutil.parser import parse as parse_date
import ast
import json

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
class Constants:
    MAX_ROWS_PER_REQUEST = 1000000
    BATCH_SIZE = 50000
    REQUEST_DELAY = 1
    MAX_RETRIES = 3
    RETRY_DELAY = 30
    MEMORY_BATCH_SIZE = 10000
    COVERAGE_THRESHOLD = 0.95
    MIN_ROWS_PER_DAY = 10

class ProcessingStatus(Enum):
    SUCCESS = "success"
    PARTIAL_SUCCESS = "partial_success"
    FAILED = "failed"
    SKIPPED = "skipped"

@dataclass
class FieldConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ–ª—è –¥–ª—è –º–∞–ø–ø–∏–Ω–≥–∞ YM -> ClickHouse"""
    field: str
    column: str
    type: str
    comment: str
    nullable: bool = False
    default_value: Any = None

@dataclass
class ProcessingResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—á–µ—Ç—á–∏–∫–∞"""
    counter_id: str
    site_name: str
    status: ProcessingStatus
    total_loaded: int = 0
    dates_processed: int = 0
    dates_skipped: int = 0
    errors: List[str] = field(default_factory=list)
    processing_time: float = 0.0

class ConfigManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ–ª–µ–π"""
    
    @staticmethod
    def get_field_config() -> List[FieldConfig]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ–ª–µ–π —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏"""
        return [
            FieldConfig("ym:s:visitID", "visit_id", "UInt64", "–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –≤–∏–∑–∏—Ç–∞"),
            FieldConfig("ym:s:counterID", "counter_id", "UInt32", "–ù–æ–º–µ—Ä —Å—á–µ—Ç—á–∏–∫–∞"),
            FieldConfig("ym:s:watchIDs", "watch_ids", "Array(UInt64)", "–ü—Ä–æ—Å–º–æ—Ç—Ä—ã –≤–∏–∑–∏—Ç–∞"),
            FieldConfig("ym:s:dateTime", "datetime", "DateTime('Europe/Moscow')", "–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è –≤–∏–∑–∏—Ç–∞"),
            FieldConfig("ym:s:isNewUser", "is_new_user", "Bool", "–ü–µ—Ä–≤—ã–π –≤–∏–∑–∏—Ç –ø–æ—Å–µ—Ç–∏—Ç–µ–ª—è"),
            FieldConfig("ym:s:startURL", "start_url", "String", "–°—Ç—Ä–∞–Ω–∏—Ü–∞ –≤—Ö–æ–¥–∞"),
            FieldConfig("ym:s:endURL", "end_url", "String", "–°—Ç—Ä–∞–Ω–∏—Ü–∞ –≤—ã—Ö–æ–¥–∞"),
            FieldConfig("ym:s:pageViews", "page_views", "UInt16", "–ì–ª—É–±–∏–Ω–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞"),
            FieldConfig("ym:s:visitDuration", "visit_duration", "UInt32", "–í—Ä–µ–º—è –Ω–∞ —Å–∞–π—Ç–µ (—Å–µ–∫)"),
            FieldConfig("ym:s:bounce", "bounce", "Bool", "–û—Ç–∫–∞–∑–Ω–æ—Å—Ç—å"),
            FieldConfig("ym:s:ipAddress", "ip_address", "String", "IP –∞–¥—Ä–µ—Å"),
            FieldConfig("ym:s:regionCountry", "region_country", "LowCardinality(String)", "–°—Ç—Ä–∞–Ω–∞ (ISO)"),
            FieldConfig("ym:s:regionCity", "region_city", "LowCardinality(String)", "–ì–æ—Ä–æ–¥"),
            FieldConfig("ym:s:clientID", "client_id", "UInt64", "–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"),
            FieldConfig("ym:s:counterUserIDHash", "counter_user_id_hash", "Nullable(UInt64)", "–•–µ—à ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è", True),
            FieldConfig("ym:s:networkType", "network_type", "LowCardinality(String)", "–¢–∏–ø —Å–µ—Ç–∏"),
            FieldConfig("ym:s:goalsID", "goals_id", "Array(UInt32)", "–¶–µ–ª–∏ –≤–∏–∑–∏—Ç–∞"),
            FieldConfig("ym:s:goalsSerialNumber", "goals_serial_number", "Array(UInt32)", "–°–µ—Ä–∏–π–Ω—ã–µ –Ω–æ–º–µ—Ä–∞ —Ü–µ–ª–µ–π"),
            FieldConfig("ym:s:goalsDateTime", "goals_datetime", "Array(DateTime('Europe/Moscow'))", "–í—Ä–µ–º—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–µ–π"),
            FieldConfig("ym:s:referer", "referer", "String", "–†–µ—Ñ–µ—Ä–µ—Ä"),
            FieldConfig("ym:s:from", "from_source", "LowCardinality(String)", "–ò—Å—Ç–æ—á–Ω–∏–∫ –ø–µ—Ä–µ—Ö–æ–¥–∞"),
            FieldConfig("ym:s:browserLanguage", "browser_language", "LowCardinality(String)", "–Ø–∑—ã–∫ –±—Ä–∞—É–∑–µ—Ä–∞"),
            FieldConfig("ym:s:browserCountry", "browser_country", "LowCardinality(String)", "–°—Ç—Ä–∞–Ω–∞ –±—Ä–∞—É–∑–µ—Ä–∞"),
            FieldConfig("ym:s:clientTimeZone", "client_timezone", "Int16", "–ß–∞—Å–æ–≤–æ–π –ø–æ—è—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"),
            FieldConfig("ym:s:deviceCategory", "device_category", "LowCardinality(String)", "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"),
            FieldConfig("ym:s:mobilePhone", "mobile_phone", "LowCardinality(String)", "–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"),
            FieldConfig("ym:s:mobilePhoneModel", "mobile_phone_model", "LowCardinality(String)", "–ú–æ–¥–µ–ª—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"),
            FieldConfig("ym:s:operatingSystemRoot", "os_root", "LowCardinality(String)", "–ì—Ä—É–ø–ø–∞ –û–°"),
            FieldConfig("ym:s:operatingSystem", "os", "LowCardinality(String)", "–û–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞"),
            FieldConfig("ym:s:browser", "browser", "LowCardinality(String)", "–ë—Ä–∞—É–∑–µ—Ä"),
            FieldConfig("ym:s:browserEngine", "browser_engine", "LowCardinality(String)", "–î–≤–∏–∂–æ–∫ –±—Ä–∞—É–∑–µ—Ä–∞"),
            FieldConfig("ym:s:cookieEnabled", "cookie_enabled", "Bool", "Cookie –≤–∫–ª—é—á–µ–Ω—ã"),
            FieldConfig("ym:s:javascriptEnabled", "javascript_enabled", "Bool", "JavaScript –≤–∫–ª—é—á—ë–Ω"),
            FieldConfig("ym:s:screenFormat", "screen_format", "LowCardinality(String)", "–§–æ—Ä–º–∞—Ç —ç–∫—Ä–∞–Ω–∞"),
            FieldConfig("ym:s:screenColors", "screen_colors", "UInt8", "–ì–ª—É–±–∏–Ω–∞ —Ü–≤–µ—Ç–∞"),
            FieldConfig("ym:s:screenOrientation", "screen_orientation", "UInt8", "–û—Ä–∏–µ–Ω—Ç–∞—Ü–∏—è —ç–∫—Ä–∞–Ω–∞"),
            FieldConfig("ym:s:screenOrientationName", "screen_orientation_name", "LowCardinality(String)", "–ù–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–∏"),
            FieldConfig("ym:s:screenWidth", "screen_width", "UInt16", "–®–∏—Ä–∏–Ω–∞ —ç–∫—Ä–∞–Ω–∞"),
            FieldConfig("ym:s:screenHeight", "screen_height", "UInt16", "–í—ã—Å–æ—Ç–∞ —ç–∫—Ä–∞–Ω–∞"),
            FieldConfig("ym:s:physicalScreenWidth", "physical_screen_width", "UInt16", "–§–∏–∑–∏—á–µ—Å–∫–∞—è —à–∏—Ä–∏–Ω–∞"),
            FieldConfig("ym:s:physicalScreenHeight", "physical_screen_height", "UInt16", "–§–∏–∑–∏—á–µ—Å–∫–∞—è –≤—ã—Å–æ—Ç–∞"),
            FieldConfig("ym:s:windowClientWidth", "window_client_width", "UInt16", "–®–∏—Ä–∏–Ω–∞ –æ–∫–Ω–∞"),
            FieldConfig("ym:s:windowClientHeight", "window_client_height", "UInt16", "–í—ã—Å–æ—Ç–∞ –æ–∫–Ω–∞"),
            FieldConfig("ym:s:parsedParamsKey1", "parsed_params_key1", "Nullable(String)", "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∏–∑–∏—Ç–∞, —É—Ä. 1", True),
        ]

class DataValidator:
    """–í–∞–ª–∏–¥–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö"""
    
    @staticmethod
    def validate_counter_id(counter_id: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è ID —Å—á–µ—Ç—á–∏–∫–∞"""
        return counter_id.isdigit() and len(counter_id) <= 20
    
    @staticmethod
    def validate_date_format(date_str: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞—Ç—ã"""
        try:
            datetime.strptime(date_str, "%Y-%m-%d")
            return True
        except ValueError:
            return False
    
    @staticmethod
    def validate_date_range(start_date: str, end_date: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –¥–∞—Ç"""
        try:
            start = datetime.strptime(start_date, "%Y-%m-%d")
            end = datetime.strptime(end_date, "%Y-%m-%d")
            return start <= end and (end - start).days <= 365  # –ú–∞–∫—Å–∏–º—É–º –≥–æ–¥
        except ValueError:
            return False

class DataNormalizer:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ClickHouse"""
    
    def __init__(self, field_configs: List[FieldConfig]):
        self.field_configs = field_configs
    
    def normalize_row(self, row: List[Any]) -> List[Any]:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–æ–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        if len(row) != len(self.field_configs):
            logger.warning(f"–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ–ª–µ–π: –ø–æ–ª—É—á–µ–Ω–æ {len(row)}, –æ–∂–∏–¥–∞–µ—Ç—Å—è {len(self.field_configs)}")
            # –î–æ–ø–æ–ª–Ω—è–µ–º –∏–ª–∏ –æ–±—Ä–µ–∑–∞–µ–º –¥–æ –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
            row = (row + [None] * len(self.field_configs))[:len(self.field_configs)]
        
        normalized = []
        
        for value, config in zip(row, self.field_configs):
            try:
                normalized_value = self._normalize_field(value, config)
                normalized.append(normalized_value)
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ–ª—è {config.column}: {e}")
                normalized.append(self._get_default_value(config))
        
        return normalized
    
    def _normalize_field(self, value: Any, config: FieldConfig) -> Any:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø–æ–ª—è"""
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ NULL –∑–Ω–∞—á–µ–Ω–∏–π
        if value is None or value == '':
            if config.nullable or "Array" in config.type:
                return None if "Array" not in config.type else []
            else:
                return self._get_default_value(config)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –º–∞—Å—Å–∏–≤–æ–≤
        if config.type.startswith("Array("):
            return self._normalize_array(value, config)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ DateTime
        elif "DateTime" in config.type:
            return self._normalize_datetime(value)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ Boolean
        elif config.type == "Bool":
            return self._normalize_boolean(value)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∏—Å–ª–æ–≤—ã—Ö —Ç–∏–ø–æ–≤
        elif any(t in config.type for t in ["UInt", "Int"]):
            return self._normalize_numeric(value, config)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–æ–∫
        else:
            return str(value) if value is not None else ""
    
    def _normalize_array(self, value: Any, config: FieldConfig) -> List[Any]:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Å—Å–∏–≤–æ–≤"""
        if isinstance(value, str) and value.strip():
            try:
                parsed_value = ast.literal_eval(value)
                if isinstance(parsed_value, list):
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ DateTime –≤ –º–∞—Å—Å–∏–≤–∞—Ö
                    if "DateTime" in config.type:
                        return [self._normalize_datetime(v) for v in parsed_value]
                    return parsed_value
            except (ValueError, SyntaxError):
                pass
        elif isinstance(value, list):
            return value
        
        return []
    
    def _normalize_datetime(self, value: Any) -> datetime:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è DateTime"""
        if isinstance(value, datetime):
            return value
        elif isinstance(value, str):
            try:
                return parse_date(value)
            except Exception:
                return datetime(1970, 1, 1)  # Unix epoch –∫–∞–∫ fallback
        else:
            return datetime(1970, 1, 1)
    
    def _normalize_boolean(self, value: Any) -> bool:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è Boolean"""
        if isinstance(value, str):
            return value.lower() in ('true', '1', 'yes', 'on')
        return bool(value)
    
    def _normalize_numeric(self, value: Any, config: FieldConfig) -> int:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö —Ç–∏–ø–æ–≤"""
        try:
            if isinstance(value, str):
                if not value.isdigit():
                    return 0
                num_value = int(value)
            else:
                num_value = int(value)
            
            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤
            if "UInt8" in config.type:
                return min(max(num_value, 0), 255)
            elif "UInt16" in config.type:
                return min(max(num_value, 0), 65535)
            elif "UInt32" in config.type:
                return min(max(num_value, 0), 4294967295)
            elif "Int16" in config.type:
                return min(max(num_value, -32768), 32767)
            
            return num_value
        except (ValueError, TypeError):
            return 0
    
    def _get_default_value(self, config: FieldConfig) -> Any:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è —Ç–∏–ø–∞"""
        if config.default_value is not None:
            return config.default_value
        
        if "Array" in config.type:
            return []
        elif config.nullable:
            return None
        elif "String" in config.type:
            return ""
        elif any(t in config.type for t in ["UInt", "Int"]):
            return 0
        elif "Bool" in config.type:
            return False
        elif "DateTime" in config.type:
            return datetime(1970, 1, 1)
        else:
            return None

class ClickHouseManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å ClickHouse"""
    
    def __init__(self):
        self.client = self._create_client()
        self.field_configs = ConfigManager.get_field_config()
    
    def _create_client(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ ClickHouse —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
        required_env_vars = [
            "CLICKHOUSE_HOST", "CLICKHOUSE_PORT", 
            "CLICKHOUSE_USER", "CLICKHOUSE_PASSWORD", "CLICKHOUSE_DB"
        ]
        
        missing_vars = [var for var in required_env_vars if not os.getenv(var)]
        if missing_vars:
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è: {missing_vars}")
        
        try:
            return clickhouse_connect.get_client(
                host=os.getenv("CLICKHOUSE_HOST"),
                port=int(os.getenv("CLICKHOUSE_PORT")),
                username=os.getenv("CLICKHOUSE_USER"),
                password=os.getenv("CLICKHOUSE_PASSWORD"),
                database=os.getenv("CLICKHOUSE_DB"),
                connect_timeout=30,
                send_receive_timeout=300
            )
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ ClickHouse: {e}")
            raise
    
    def create_table(self, table_name: str = "ym_visits") -> None:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å—Ö–µ–º–æ–π"""
        columns_sql = ",\n    ".join(
            f"{config.column} {config.type} COMMENT '{config.comment}'"
            for config in self.field_configs
        )
        
        create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS dev.{table_name} (
            {columns_sql},
            site_name LowCardinality(String) COMMENT '–ù–∞–∑–≤–∞–Ω–∏–µ —Å–∞–π—Ç–∞',
            load_date Date DEFAULT today() COMMENT '–î–∞—Ç–∞ –∑–∞–≥—Ä—É–∑–∫–∏',
            load_timestamp DateTime DEFAULT now() COMMENT '–í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏'
        ) ENGINE = MergeTree()
        PARTITION BY toYYYYMM(datetime)
        ORDER BY (counter_id, datetime, visit_id)
        SETTINGS index_granularity = 8192
        COMMENT '–ï–¥–∏–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –≤–∏–∑–∏—Ç–æ–≤ –Ø–Ω–¥–µ–∫—Å.–ú–µ—Ç—Ä–∏–∫–∏'
        """
        
        try:
            self.client.command(create_table_sql)
            logger.info(f"–¢–∞–±–ª–∏—Ü–∞ {table_name} —Å–æ–∑–¥–∞–Ω–∞/–ø—Ä–æ–≤–µ—Ä–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã {table_name}: {e}")
            raise
    
    def get_existing_dates(self, counter_id: str, start_date: str, end_date: str, 
                          table_name: str = "ym_visits") -> set:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞—Ç —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º"""
        query = f"""
        SELECT DISTINCT toDate(datetime) as date
        FROM dev.{table_name}
        WHERE counter_id = %(counter_id)s
          AND datetime >= %(start_date)s
          AND datetime <= %(end_date)s
        ORDER BY date
        """
        
        try:
            result = self.client.query(
                query,
                parameters={
                    'counter_id': int(counter_id),
                    'start_date': start_date,
                    'end_date': f"{end_date} 23:59:59"
                }
            )
            return {str(row[0]) for row in result.result_rows}
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞—Ç: {e}")
            return set()
    
    def get_incomplete_dates(self, counter_id: str, start_date: str, end_date: str,
                           table_name: str = "ym_visits") -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç —Å –Ω–µ–ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π"""
        query = f"""
        WITH date_stats AS (
            SELECT 
                toDate(datetime) as date,
                count() as row_count,
                min(visit_id) as min_visit_id,
                max(visit_id) as max_visit_id,
                max(visit_id) - min(visit_id) + 1 as expected_range,
                countDistinct(visit_id) as unique_visits
            FROM dev.{table_name}
            WHERE counter_id = %(counter_id)s
            AND datetime >= %(start_date)s
            AND datetime <= %(end_date)s
            GROUP BY toDate(datetime)
        )
        SELECT date
        FROM date_stats
        WHERE unique_visits < expected_range * %(coverage_threshold)s
        OR row_count < %(min_rows)s
        ORDER BY date
        """
        
        try:
            result = self.client.query(
                query,
                parameters={
                    'counter_id': int(counter_id),
                    'start_date': start_date,
                    'end_date': f"{end_date} 23:59:59",
                    'coverage_threshold': Constants.COVERAGE_THRESHOLD,
                    'min_rows': Constants.MIN_ROWS_PER_DAY
                }
            )
            return [str(row[0]) for row in result.result_rows]
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–µ–ø–æ–ª–Ω—ã—Ö –¥–∞—Ç: {e}")
            return []
    
    def delete_date_data(self, counter_id: str, date: str, table_name: str = "ym_visits") -> None:
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∑–∞ –¥–∞—Ç—É —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º"""
        query = f"""
        DELETE FROM dev.{table_name} 
        WHERE counter_id = %(counter_id)s AND toDate(datetime) = %(date)s
        """
        
        try:
            self.client.command(
                query,
                parameters={
                    'counter_id': int(counter_id),
                    'date': date
                }
            )
            logger.info(f"–£–¥–∞–ª–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—á–µ—Ç—á–∏–∫–∞ {counter_id} –∑–∞ {date}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}")
            raise
    
    def insert_data(self, data: List[List[Any]], site_name: str, 
                   table_name: str = "ym_visits") -> int:
        """–í—Å—Ç–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        if not data:
            return 0
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –ø–æ–ª—è
        enhanced_data = []
        current_time = datetime.now()
        current_date = current_time.date()
        
        for row in data:
            enhanced_row = row + [site_name, current_date, current_time]
            enhanced_data.append(enhanced_row)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫
        column_names = [config.column for config in self.field_configs]
        extended_columns = column_names + ['site_name', 'load_date', 'load_timestamp']
        
        try:
            self.client.insert(
                table=f"dev.{table_name}",
                data=enhanced_data,
                column_names=extended_columns
            )
            logger.info(f"–£—Å–ø–µ—à–Ω–æ –≤—Å—Ç–∞–≤–ª–µ–Ω–æ {len(enhanced_data)} —Å—Ç—Ä–æ–∫")
            return len(enhanced_data)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—Å—Ç–∞–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
            raise 

class YandexMetrikaClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å API –Ø–Ω–¥–µ–∫—Å.–ú–µ—Ç—Ä–∏–∫–∏"""
    
    def __init__(self):
        self.access_token = os.getenv("YM_TOKEN")
        if not self.access_token:
            raise ValueError("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ç–æ–∫–µ–Ω YM_TOKEN")
        
        self.field_configs = ConfigManager.get_field_config()
        self.fields = [config.field for config in self.field_configs]
    
    def fetch_data_for_date(self, counter_id: str, date: str) -> List[List[Any]]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞ –¥–∞—Ç—É —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        if not DataValidator.validate_counter_id(counter_id):
            raise ValueError(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π ID —Å—á–µ—Ç—á–∏–∫–∞: {counter_id}")
        
        if not DataValidator.validate_date_format(date):
            raise ValueError(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã: {date}")
        
        client = YandexMetrikaLogsapi(
            access_token=self.access_token,
            default_url_params={"counterId": counter_id},
            wait_report=True
        )
        
        params = {
            "fields": ",".join(self.fields),
            "source": "visits",
            "date1": date,
            "date2": date
        }
        
        for attempt in range(Constants.MAX_RETRIES):
            try:
                logger.info(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—á–µ—Ç—á–∏–∫–∞ {counter_id} –∑–∞ {date}")
                
                # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å
                result = client.create().post(params=params)
                request_id = result["log_request"]["request_id"]
                
                # –ñ–¥–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                report = client.download(requestId=request_id).get()
                data = report().to_values()
                
                if not data:
                    logger.info(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—á–µ—Ç—á–∏–∫–∞ {counter_id} –∑–∞ {date}")
                    return []
                
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} —Å—Ç—Ä–æ–∫ –¥–ª—è —Å—á–µ—Ç—á–∏–∫–∞ {counter_id} –∑–∞ {date}")
                return data
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
                if attempt < Constants.MAX_RETRIES - 1:
                    sleep_time = Constants.RETRY_DELAY * (attempt + 1)
                    logger.info(f"–û–∂–∏–¥–∞–Ω–∏–µ {sleep_time} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º")
                    time.sleep(sleep_time)
                else:
                    logger.error(f"–í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã –¥–ª—è —Å—á–µ—Ç—á–∏–∫–∞ {counter_id} –∑–∞ {date}")
                    raise
        
        return []

class YandexMetrikaETL:
    """–û—Å–Ω–æ–≤–Ω–æ–π ETL –∫–ª–∞—Å—Å —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π"""
    
    def __init__(self):
        self.ch_manager = ClickHouseManager()
        self.ym_client = YandexMetrikaClient()
        self.normalizer = DataNormalizer(ConfigManager.get_field_config())
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        try:
            from utils.telegram_notify import TelegramNotifier
            self.notifier = TelegramNotifier(debug_mode=True)
        except ImportError:
            logger.warning("TelegramNotifier –Ω–µ –Ω–∞–π–¥–µ–Ω, —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ—Ç–∫–ª—é—á–µ–Ω—ã")
            self.notifier = None
    
    @contextmanager
    def memory_management(self):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é"""
        try:
            yield
        finally:
            gc.collect()
    
    def _generate_date_range(self, start_date: str, end_date: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ø–∏—Å–∫–∞ –¥–∞—Ç –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
        if not DataValidator.validate_date_range(start_date, end_date):
            raise ValueError(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç: {start_date} - {end_date}")
        
        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
        end_dt = datetime.strptime(end_date, "%Y-%m-%d")
        dates = []
        
        current_dt = start_dt
        while current_dt <= end_dt:
            dates.append(current_dt.strftime("%Y-%m-%d"))
            current_dt += timedelta(days=1)
        
        return dates
    
    def get_dates_to_process(self, counter_id: str, start_date: str, end_date: str) -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        # –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞—Ç—ã
        existing_dates = self.ch_manager.get_existing_dates(counter_id, start_date, end_date)
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç—ã —Å –Ω–µ–ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π
        incomplete_dates = self.ch_manager.get_incomplete_dates(counter_id, start_date, end_date)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Å–µ –¥–∞—Ç—ã –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ
        all_dates = set(self._generate_date_range(start_date, end_date))
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∞—Ç—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        missing_dates = all_dates - existing_dates
        dates_to_process = list(missing_dates.union(set(incomplete_dates)))
        dates_to_process.sort()
        
        logger.info(f"–í—Å–µ–≥–æ –¥–∞—Ç –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ: {len(all_dates)}")
        logger.info(f"–°—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞—Ç: {len(existing_dates)}")
        logger.info(f"–î–∞—Ç —Å –Ω–µ–ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π: {len(incomplete_dates)}")
        logger.info(f"–î–∞—Ç –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {len(dates_to_process)}")
        
        return dates_to_process
    
    def process_date(self, counter_id: str, date: str, site_name: str) -> Tuple[int, List[str]]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞ –æ–¥–Ω—É –¥–∞—Ç—É"""
        errors = []
        
        try:
            with self.memory_management():
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
                existing_dates = self.ch_manager.get_existing_dates(counter_id, date, date)
                
                if date in existing_dates:
                    logger.info(f"–î–∞—Ç–∞ {date}: –Ω–∞–π–¥–µ–Ω—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ, –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞")
                    self.ch_manager.delete_date_data(counter_id, date)
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ API
                raw_data = self.ym_client.fetch_data_for_date(counter_id, date)
                
                if not raw_data:
                    logger.info(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—á–µ—Ç—á–∏–∫–∞ {counter_id} –∑–∞ {date}")
                    return 0, errors
                
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ—Ä—Ü–∏—è–º–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                total_inserted = 0
                
                for i in range(0, len(raw_data), Constants.MEMORY_BATCH_SIZE):
                    batch = raw_data[i:i + Constants.MEMORY_BATCH_SIZE]
                    
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –±–∞—Ç—á
                    normalized_batch = [self.normalizer.normalize_row(row) for row in batch]
                    
                    # –í—Å—Ç–∞–≤–ª—è–µ–º –≤ ClickHouse
                    inserted = self.ch_manager.insert_data(normalized_batch, site_name)
                    total_inserted += inserted
                    
                    # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
                    del batch, normalized_batch
                    
                    if i % (Constants.MEMORY_BATCH_SIZE * 5) == 0:
                        gc.collect()
                
                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {total_inserted} —Å—Ç—Ä–æ–∫ –¥–ª—è —Å—á–µ—Ç—á–∏–∫–∞ {counter_id} –∑–∞ {date}")
                return total_inserted, errors
                
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞—Ç—ã {date}: {e}"
            logger.error(error_msg)
            errors.append(error_msg)
            return 0, errors
    
    def process_counter(self, counter_id: str, site_name: str, 
                       start_date: str, end_date: str) -> ProcessingResult:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—á–µ—Ç—á–∏–∫–∞ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å—é"""
        start_time = time.time()
        
        logger.info(f"–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—á–µ—Ç—á–∏–∫–∞ {counter_id} ({site_name})")
        
        result = ProcessingResult(
            counter_id=counter_id,
            site_name=site_name,
            status=ProcessingStatus.FAILED
        )
        
        try:
            # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            self.ch_manager.create_table()
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∞—Ç—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            dates_to_process = self.get_dates_to_process(counter_id, start_date, end_date)
            
            if not dates_to_process:
                logger.info(f"–í—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—á–µ—Ç—á–∏–∫–∞ {counter_id} –∑–∞–≥—Ä—É–∂–µ–Ω—ã –ø–æ–ª–Ω–æ—Å—Ç—å—é")
                result.status = ProcessingStatus.SKIPPED
                total_days = (datetime.strptime(end_date, "%Y-%m-%d") - 
                            datetime.strptime(start_date, "%Y-%m-%d")).days + 1
                result.dates_skipped = total_days
                return result
            
            logger.info(f"–¢—Ä–µ–±—É–µ—Ç—Å—è –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å {len(dates_to_process)} –¥–∞—Ç")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –¥–∞—Ç—É
            for date in dates_to_process:
                try:
                    inserted, date_errors = self.process_date(counter_id, date, site_name)
                    
                    result.total_loaded += inserted
                    result.dates_processed += 1
                    result.errors.extend(date_errors)
                    
                    # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    time.sleep(Constants.REQUEST_DELAY)
                    
                except Exception as e:
                    error_msg = f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞—Ç—ã {date}: {e}"
                    logger.error(error_msg)
                    result.errors.append(error_msg)
                    continue
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            if not result.errors:
                result.status = ProcessingStatus.SUCCESS
            elif result.total_loaded > 0:
                result.status = ProcessingStatus.PARTIAL_SUCCESS
            else:
                result.status = ProcessingStatus.FAILED
            
            result.processing_time = time.time() - start_time
            
            logger.info(f"–ó–∞–≤–µ—Ä—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—á–µ—Ç—á–∏–∫–∞ {counter_id}. "
                       f"–°—Ç–∞—Ç—É—Å: {result.status.value}, "
                       f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ: {result.total_loaded} —Å—Ç—Ä–æ–∫, "
                       f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–∞—Ç: {result.dates_processed}, "
                       f"–û—à–∏–±–æ–∫: {len(result.errors)}, "
                       f"–í—Ä–µ–º—è: {result.processing_time:.2f}—Å")
            
        except Exception as e:
            error_msg = f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—á–µ—Ç—á–∏–∫–∞ {counter_id}: {e}"
            logger.error(error_msg)
            result.errors.append(error_msg)
            result.status = ProcessingStatus.FAILED
            result.processing_time = time.time() - start_time
        
        return result
    
    def send_notification(self, message: str, is_error: bool = False) -> None:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è"""
        if self.notifier:
            try:
                self.notifier.send_message(message, is_error=is_error)
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è: {e}")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ETL
etl = YandexMetrikaETL()

def process_single_counter(counter_id: str, site_name: str, **context) -> Dict[str, Any]:
    """–ó–∞–¥–∞—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ —Å—á–µ—Ç—á–∏–∫–∞"""
    try:
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ Airflow Variables
        days_back = int(Variable.get("ym_days_back", default_var=7))
        start_date = (datetime.utcnow() - timedelta(days=days_back)).strftime("%Y-%m-%d")
        end_date = (datetime.utcnow() - timedelta(days=1)).strftime("%Y-%m-%d")
        
        logger.info(f"–ü–µ—Ä–∏–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏: {start_date} - {end_date}")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if not DataValidator.validate_counter_id(counter_id):
            raise ValueError(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π ID —Å—á–µ—Ç—á–∏–∫–∞: {counter_id}")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫
        result = etl.process_counter(counter_id, site_name, start_date, end_date)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
        status_emoji = {
            ProcessingStatus.SUCCESS: "‚úÖ",
            ProcessingStatus.PARTIAL_SUCCESS: "‚ö†Ô∏è",
            ProcessingStatus.FAILED: "‚ùå",
            ProcessingStatus.SKIPPED: "‚è≠Ô∏è"
        }
        
        emoji = status_emoji.get(result.status, "‚ùì")
        message = (f"{emoji} –°—á–µ—Ç—á–∏–∫ {counter_id} ({site_name}):\n"
                  f"–°—Ç–∞—Ç—É—Å: {result.status.value}\n"
                  f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ: {result.total_loaded:,} —Å—Ç—Ä–æ–∫\n"
                  f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–∞—Ç: {result.dates_processed}\n"
                  f"–í—Ä–µ–º—è: {result.processing_time:.2f}—Å")
        
        if result.errors:
            message += f"\n–û—à–∏–±–æ–∫: {len(result.errors)}"
        
        etl.send_notification(message, is_error=(result.status == ProcessingStatus.FAILED))
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è XCom
        return {
            'counter_id': result.counter_id,
            'site_name': result.site_name,
            'status': result.status.value,
            'total_loaded': result.total_loaded,
            'dates_processed': result.dates_processed,
            'dates_skipped': result.dates_skipped,
            'errors': result.errors,
            'processing_time': result.processing_time,
            'success': result.status in [ProcessingStatus.SUCCESS, ProcessingStatus.SKIPPED]
        }
        
    except Exception as e:
        error_msg = f"‚õîÔ∏è –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –∑–∞–¥–∞—á–µ {counter_id}: {e}"
        logger.error(error_msg)
        etl.send_notification(error_msg, is_error=True)
        raise

def get_data_quality_report(**context) -> None:
    """–û—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    try:
        days_back = int(Variable.get("ym_days_back", default_var=7))
        start_date = (datetime.utcnow() - timedelta(days=days_back)).strftime("%Y-%m-%d")
        end_date = (datetime.utcnow() - timedelta(days=1)).strftime("%Y-%m-%d")
        
        quality_query = f"""
        WITH date_quality AS (
            SELECT 
                counter_id,
                toDate(datetime) as date,
                count() as row_count,
                countDistinct(visit_id) as unique_visits,
                min(visit_id) as min_visit,
                max(visit_id) as max_visit,
                CASE 
                    WHEN max(visit_id) - min(visit_id) = 0 THEN 100
                    ELSE countDistinct(visit_id) / (max(visit_id) - min(visit_id) + 1) * 100
                END as coverage_percent
            FROM dev.ym_visits
            WHERE datetime >= %(start_date)s AND datetime <= %(end_date)s
            GROUP BY counter_id, toDate(datetime)
        )
        SELECT 
            counter_id,
            date,
            row_count,
            coverage_percent
        FROM date_quality
        WHERE coverage_percent < %(threshold)s
        ORDER BY counter_id, date
        """
        
        quality_issues = etl.ch_manager.client.query(
            quality_query,
            parameters={
                'start_date': start_date,
                'end_date': f"{end_date} 23:59:59",
                'threshold': Constants.COVERAGE_THRESHOLD * 100
            }
        ).result_rows
        
        if quality_issues:
            report = "‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã —Å –ø–æ–ª–Ω–æ—Ç–æ–π –¥–∞–Ω–Ω—ã—Ö:\n"
            for counter_id, date, rows, coverage in quality_issues:
                report += f"–°—á–µ—Ç—á–∏–∫ {counter_id}, {date}: {rows} —Å—Ç—Ä–æ–∫, –ø–æ–∫—Ä—ã—Ç–∏–µ {coverage:.1f}%\n"
            
            etl.send_notification(report, is_error=True)
        else:
            logger.info("–í—Å–µ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —Å —Ö–æ—Ä–æ—à–∏–º –ø–æ–∫—Ä—ã—Ç–∏–µ–º visit_id")
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö: {e}")

def consolidate_results(**context) -> None:
    """–ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤—Å–µ—Ö —Å—á–µ—Ç—á–∏–∫–æ–≤"""
    task_ids = [f"process_counter_{counter['id']}" for counter in COUNTERS]
    
    results = []
    total_loaded = 0
    successful_counters = 0
    processing_times = []
    
    for task_id in task_ids:
        try:
            result = context['ti'].xcom_pull(task_ids=task_id)
            if result:
                results.append(result)
                total_loaded += result.get('total_loaded', 0)
                if result.get('success', False):
                    successful_counters += 1
                processing_times.append(result.get('processing_time', 0))
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç {task_id}: {e}")
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    total_counters = len(COUNTERS)
    failed_counters = total_counters - successful_counters
    avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0
    
    summary = (f"üìä –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –Ø–Ω–¥–µ–∫—Å.–ú–µ—Ç—Ä–∏–∫–∏:\n"
              f"–í—Å–µ–≥–æ —Å—á–µ—Ç—á–∏–∫–æ–≤: {total_counters}\n"
              f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {successful_counters}\n"
              f"‚ö†Ô∏è –° –æ—à–∏–±–∫–∞–º–∏: {failed_counters}\n"
              f"üìà –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫: {total_loaded:,}\n"
              f"‚è±Ô∏è –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {avg_processing_time:.2f}—Å")
    
    # –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –∫–∞–∂–¥–æ–º—É —Å—á–µ—Ç—á–∏–∫—É
    details = "\n\n–î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è:"
    for result in results:
        status_emoji = "‚úÖ" if result['success'] else "‚ö†Ô∏è"
        details += (f"\n{status_emoji} {result['site_name']} ({result['counter_id']}): "
                   f"{result['total_loaded']:,} —Å—Ç—Ä–æ–∫ –∑–∞ {result['processing_time']:.1f}—Å")
        
        if result.get('errors'):
            details += f" - {len(result['errors'])} –æ—à–∏–±–æ–∫"
    
    final_message = summary + details
    etl.send_notification(final_message, is_error=failed_counters > 0)
    
    # –õ–æ–≥–∏—Ä—É–µ–º –≤ Airflow
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –£—Å–ø–µ—à–Ω–æ: {successful_counters}/{total_counters}, "
               f"–≤—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {total_loaded}, —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {avg_processing_time:.2f}—Å")

def cleanup_old_data(**context) -> None:
    """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é"""
    try:
        retention_days = int(Variable.get("ym_retention_days", default_var=365))
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑—É–º–Ω–æ—Å—Ç—å –ø–µ—Ä–∏–æ–¥–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è
        if retention_days < 30:
            logger.warning(f"–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –ø–µ—Ä–∏–æ–¥ —Ö—Ä–∞–Ω–µ–Ω–∏—è: {retention_days} –¥–Ω–µ–π. –ú–∏–Ω–∏–º—É–º 30 –¥–Ω–µ–π.")
            return
        
        cutoff_date = (datetime.utcnow() - timedelta(days=retention_days)).strftime("%Y-%m-%d")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞ –æ—á–∏—Å—Ç–∫–∞
        if Variable.get("ym_enable_cleanup", default_var="false").lower() == "true":
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
            count_query = """
            SELECT count() as rows_to_delete
            FROM dev.ym_visits 
            WHERE datetime < %(cutoff_date)s
            """
            
            rows_to_delete = etl.ch_manager.client.query(
                count_query,
                parameters={'cutoff_date': cutoff_date}
            ).first_row[0]
            
            if rows_to_delete > 0:
                delete_query = """
                DELETE FROM dev.ym_visits 
                WHERE datetime < %(cutoff_date)s
                """
                
                etl.ch_manager.client.command(
                    delete_query,
                    parameters={'cutoff_date': cutoff_date}
                )
                
                logger.info(f"–£–¥–∞–ª–µ–Ω–æ {rows_to_delete:,} —Å—Ç—Ä–æ–∫ —Å—Ç–∞—Ä—à–µ {cutoff_date}")
                etl.send_notification(f"üóëÔ∏è –û—á–∏—Å—Ç–∫–∞: —É–¥–∞–ª–µ–Ω–æ {rows_to_delete:,} —Å—Ç—Ä–æ–∫ —Å—Ç–∞—Ä—à–µ {cutoff_date}")
            else:
                logger.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è")
        else:
            logger.info("–û—á–∏—Å—Ç–∫–∞ –æ—Ç–∫–ª—é—á–µ–Ω–∞ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö")
            
    except Exception as e:
        error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}"
        logger.error(error_msg)
        etl.send_notification(error_msg, is_error=True)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—á–µ—Ç—á–∏–∫–æ–≤
COUNTERS = [
    {"id": "96232424", "site": "mediiia.com"}, 
    {"id": "93017305", "site": "deziiign.com"}, 
    {"id": "93017345", "site": "gallllery.com"}
]

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ DAG
default_args = {
    'owner': 'data_team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=15),
    'retry_exponential_backoff': True,
    'max_retry_delay': timedelta(hours=1)
}

# –°–æ–∑–¥–∞–Ω–∏–µ DAG
with DAG(
    dag_id='yandex_metrika_v2',
    default_args=default_args,
    description='–†–µ—Ñ–∞–∫—Ç–æ—Ä–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –Ø–Ω–¥–µ–∫—Å.–ú–µ—Ç—Ä–∏–∫–∏ v2.0',
    schedule_interval='0 6 * * *',
    catchup=False,
    max_active_runs=1,
    tags=['etl', 'yandex_metrika', 'clickhouse', 'v2'],
    doc_md="""
    # –†–µ—Ñ–∞–∫—Ç–æ—Ä–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –Ø–Ω–¥–µ–∫—Å.–ú–µ—Ç—Ä–∏–∫–∏ v2.0
    
    ## –ö–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
    - ‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω—ã SQL-–∏–Ω—ä–µ–∫—Ü–∏–∏ (–ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã)
    - ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    - ‚úÖ –£–ª—É—á—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    - ‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ —Ä–∞–±–æ—Ç–∞ —Å –ø–∞–º—è—Ç—å—é
    - ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
    - ‚úÖ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏
    - ‚úÖ –ë–æ–ª–µ–µ –±–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    - ‚úÖ –î–µ—Ç–∞–ª—å–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    
    ## –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ:
    - `ym_days_back`: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –Ω–∞–∑–∞–¥ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 7)
    - `ym_retention_days`: —Å—Ä–æ–∫ —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –¥–Ω—è—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 365, –º–∏–Ω–∏–º—É–º 30)
    - `ym_enable_cleanup`: –≤–∫–ª—é—á–∏—Ç—å –∞–≤—Ç–æ–æ—á–∏—Å—Ç–∫—É —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é false)
    
    ## –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è:
    - `YM_TOKEN`: —Ç–æ–∫–µ–Ω –¥–æ—Å—Ç—É–ø–∞ –∫ API –Ø–Ω–¥–µ–∫—Å.–ú–µ—Ç—Ä–∏–∫–∏
    - `CLICKHOUSE_HOST`, `CLICKHOUSE_PORT`, `CLICKHOUSE_USER`, `CLICKHOUSE_PASSWORD`, `CLICKHOUSE_DB`
    """
) as dag:

    # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–¥–∞—á –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å—á–µ—Ç—á–∏–∫–∞
    counter_tasks = []
    
    for counter in COUNTERS:
        task = PythonOperator(
            task_id=f"process_counter_{counter['id']}",
            python_callable=process_single_counter,
            op_kwargs={
                "counter_id": counter["id"],
                "site_name": counter["site"]
            },
            execution_timeout=timedelta(hours=3),
            pool='yandex_metrika_pool',
            doc_md=f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—á–µ—Ç—á–∏–∫–∞ {counter['id']} ({counter['site']})"
        )
        counter_tasks.append(task)

    # –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    consolidate_task = PythonOperator(
        task_id='consolidate_results',
        python_callable=consolidate_results,
        trigger_rule=TriggerRule.ALL_DONE,
        doc_md="–ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö —Å—á–µ—Ç—á–∏–∫–æ–≤"
    )

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
    quality_check_task = PythonOperator(
        task_id='data_quality_check',
        python_callable=get_data_quality_report,
        trigger_rule=TriggerRule.ALL_DONE,
        doc_md="–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ –ø–æ–∫—Ä—ã—Ç–∏—é visit_id"
    )

    # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    cleanup_task = PythonOperator(
        task_id='cleanup_old_data',
        python_callable=cleanup_old_data,
        trigger_rule=TriggerRule.ALL_DONE,
        doc_md="–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö)"
    )

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    counter_tasks >> consolidate_task >> quality_check_task >> cleanup_task 